{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEEe_S5Jt1eK"
      },
      "source": [
        "This notebook shows the GRPO fine-tuning for sp-struct-rwd1.\n",
        "\n",
        "The system prompt can be replaced by sp-base, sp-declare, and sp-reflect, and the reward design can be replaced by reward suite 2 and reward suite 3."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Unsloth and VLLM"
      ],
      "metadata": {
        "id": "7-MOOi_XGOUa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LhzQmQX0KRT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFBsAUXNA9rJ"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install transformers==4.51.3\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import wandb"
      ],
      "metadata": {
        "id": "bmJLjMeqGRdy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScgNw4tpDfby",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# 1. Log in to wandb.\n",
        "# If you have your wandb key in an environment variable, you can skip passing it here.\n",
        "wandb.login()\n",
        "\n",
        "# 2. Initialize your run with a specific project name (and entity if needed).\n",
        "wandb.init(\n",
        "    project=\"gsm8k-prolog-prover\",\n",
        "    #entity=\"my-team-or-username\",  # optional if on personal account\n",
        "    name=\"sp-struct-rwd1\"   # optional name\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "Hqw9DDya8HO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYvRXkiG0YXc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from unsloth import is_bfloat16_supported, FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = 64, # Larger rank = smarter, but slower\n",
        "    gpu_memory_utilization = 0.7, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ], # Remove QKVO if out of memory\n",
        "    lora_alpha = 64, # LoRA rank\n",
        "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install SWI-Prolog"
      ],
      "metadata": {
        "id": "4CS4DSK7GTap"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq9Rhns-0aCv"
      },
      "outputs": [],
      "source": [
        "!apt-get install swi-prolog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq_EZ49Agx1M"
      },
      "source": [
        "### System prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec9SEnHyVb4G"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a specialized Prolog code-generating assistant.\n",
        "\n",
        "Your task is to solve math problems by providing a structured answer in two clearly defined sections:\n",
        "\n",
        "1. <reasoning>\n",
        "   - Provide a clear, concise step-by-step explanation of how you arrive at the solution.\n",
        "\n",
        "2. <answer>\n",
        "   - Provide executable Prolog code using constraint logic programming to compute the numeric answer.\n",
        "   - Always start with: ':- use_module(library(clpq)).'\n",
        "   - Define any necessary numeric constants or intermediate values using predicates.\n",
        "   - Final answer should be unified explicitly in solve(X) using curly-brace constraints, without printing commands.\n",
        "\n",
        "Use this XML format strictly:\n",
        "<reasoning>\n",
        "(Your step-by-step reasoning here)\n",
        "</reasoning>\n",
        "<answer>\n",
        ":- use_module(library(clpq)).\n",
        "\n",
        "(Any predicates/constants defined here)\n",
        "\n",
        "solve(X) :-\n",
        "    (Intermediate computations using curly braces)\n",
        "    {X = final constraint logic}.\n",
        "</answer>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWauTSEeDkC9"
      },
      "source": [
        "### Preprocess dataset and push to HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E55C30q3g9VG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from datasets import load_dataset\n",
        "import subprocess\n",
        "\n",
        "def execute_prolog_code(prolog_code: str) -> str:\n",
        "    \"\"\"\n",
        "    Executes the given Prolog code in SWI-Prolog, calling solve(X),\n",
        "    and returns the printed solution as a string (e.g., \"12000\").\n",
        "    Returns None if there's an error or no output.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Write the Prolog code to a temporary file\n",
        "        with open(\"temp.pl\", \"w\") as f:\n",
        "            f.write(prolog_code)\n",
        "\n",
        "        # Run SWI-Prolog: load 'temp.pl', call solve(X), print X, then halt\n",
        "        result = subprocess.run(\n",
        "            [\"swipl\", \"-q\", \"-f\", \"temp.pl\", \"-g\", \"solve(X), writeln(X), halt\"],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=5,  # optional: 5-second timeout\n",
        "        )\n",
        "\n",
        "        # If there's any error output, we can check result.stderr or result.returncode\n",
        "        if result.returncode != 0 or not result.stdout:\n",
        "            return None\n",
        "\n",
        "        # result.stdout is whatever got printed by writeln(X)\n",
        "        lines = result.stdout.strip().splitlines()\n",
        "        return lines[-1].strip() if lines else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing Prolog code: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCizrIcZg_3X"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def get_gsm8k_questions(split=\"train\"):\n",
        "    data = load_dataset('niklasm222/gsm8k-prolog-prover')[split]\n",
        "\n",
        "    def map_fn(x):\n",
        "        # Compute the correct numerical result by executing the reference Prolog solution.\n",
        "        numerical_result = execute_prolog_code(x[\"output\"])\n",
        "        return {\n",
        "            \"instruction\": x[\"instruction\"],\n",
        "            \"input\": x[\"input\"],\n",
        "            \"output\": x[\"output\"],\n",
        "            \"prompt\": [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": f\"{x['instruction']}\\n{x['input']}\"}\n",
        "            ],\n",
        "            # Optionally, you can also append the numerical result to the output field.\n",
        "            \"answer\": x['output'],\n",
        "            \"numerical_result\": str(numerical_result),  # Precomputed numeric result\n",
        "        }\n",
        "\n",
        "    data = data.map(map_fn)\n",
        "    return data\n",
        "\n",
        "dataset = get_gsm8k_questions()\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxyyBwfDhCBV"
      },
      "outputs": [],
      "source": [
        "# Save and push the dataset to Hugging Face Hub.\n",
        "# Replace \"your_username\" with your HF username and \"hf_your_token\" with your token if needed.\n",
        "dataset.push_to_hub(\"niklasm222/gsm8k-prolog-prover-sp_struct-v4\", token=\"\", private=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZop2dGohEsy"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIW598Nh0f57"
      },
      "outputs": [],
      "source": [
        "# FOR SUBSET TRAINING\n",
        "\n",
        "import re\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import subprocess\n",
        "\n",
        "def get_gsm8k_split(subset_size=2500, seed=42):\n",
        "    \"\"\"\n",
        "    Load the 'niklasm222/gsm8k-prolog-prover-v4' dataset, select a subset,\n",
        "    and split it into 70% train, 15% validation, and 15% test.\n",
        "    \"\"\"\n",
        "    # 1. Load dataset and shuffle\n",
        "    dataset = load_dataset(\"niklasm222/gsm8k-prolog-prover-sp_struct-v4\", split=\"train\")\n",
        "    subset = dataset.shuffle(seed=seed).select(range(subset_size))\n",
        "\n",
        "    # 2. Split off 15% for test\n",
        "    split_1 = subset.train_test_split(test_size=0.15, seed=seed)\n",
        "    train_val = split_1[\"train\"]\n",
        "    test = split_1[\"test\"]\n",
        "\n",
        "    # 3. From the remaining 85%, split off 15% for validation (~0.1765)\n",
        "    val_ratio = 0.15 / 0.85\n",
        "    split_2 = train_val.train_test_split(test_size=val_ratio, seed=seed)\n",
        "    train = split_2[\"train\"]\n",
        "    val = split_2[\"test\"]\n",
        "\n",
        "    return DatasetDict({\"train\": train, \"validation\": val, \"test\": test})\n",
        "\n",
        "# Load Data\n",
        "splits = get_gsm8k_split()\n",
        "train_dataset = splits[\"train\"]\n",
        "val_dataset = splits[\"validation\"]\n",
        "test_dataset = splits[\"test\"]\n",
        "\n",
        "# Print dataset information\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "print(f\"Columns: {train_dataset.column_names}\")\n",
        "\n",
        "# Inspect the first training sample\n",
        "print(\"\\nTraining sample:\")\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "AilVh2fl5pL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------\n",
        "# Helper Functions\n",
        "# ---------------------\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    \"\"\"\n",
        "    1) Truncate 'text' at <|endoftext|> if present.\n",
        "    2) Find the FIRST fully-completed <answer>...</answer> block in that truncated text.\n",
        "    3) Return that block's content, or None if not found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1) Truncate at <|endoftext|>\n",
        "        eot_index = text.find(\"<|endoftext|>\")\n",
        "        truncated_text = text[:eot_index] if eot_index != -1 else text\n",
        "\n",
        "        # 2) Find the FIRST <answer> tag\n",
        "        start = truncated_text.find(\"<answer>\")\n",
        "        if start == -1:\n",
        "            return None\n",
        "\n",
        "        # 3) Find the NEXT </answer> after this <answer>\n",
        "        end = truncated_text.find(\"</answer>\", start)\n",
        "        if end == -1:\n",
        "            return None\n",
        "\n",
        "        return truncated_text[start+len(\"<answer>\"):end].strip()\n",
        "\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def execute_prolog_code(prolog_code: str) -> str:\n",
        "    \"\"\"\n",
        "    Executes the given Prolog code in SWI-Prolog, calling solve(X),\n",
        "    and returns the printed solution as a string (e.g., \"12000\").\n",
        "    Returns None if there's an error or no output.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Write the Prolog code to a temporary file\n",
        "        with open(\"temp.pl\", \"w\") as f:\n",
        "            f.write(prolog_code)\n",
        "\n",
        "        # Run SWI-Prolog: load 'temp.pl', call solve(X), print X, then halt\n",
        "        result = subprocess.run(\n",
        "            [\"swipl\", \"-q\", \"-f\", \"temp.pl\", \"-g\", \"solve(X), writeln(X), halt\"],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=5,  # optional: 5-second timeout\n",
        "        )\n",
        "\n",
        "        # If there's any error output, we can check result.stderr or result.returncode\n",
        "        if result.returncode != 0 or not result.stdout:\n",
        "            return None\n",
        "\n",
        "        # result.stdout is whatever got printed by writeln(X)\n",
        "        lines = result.stdout.strip().splitlines()\n",
        "        return lines[-1].strip() if lines else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing Prolog code: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "8TouRF52vilH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reward design (reward suite 1)"
      ],
      "metadata": {
        "id": "Isp6vBr35rRF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55vkXQCvbjcv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import uuid\n",
        "import subprocess\n",
        "from datasets import load_dataset\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, numerical_result, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Compare the modelâ€™s executed Prolog answer to the known correct numeric result.\n",
        "    Provide partial rewards for progress toward correctness during early training.\n",
        "    This function depends on SWI-Prolog execution results.\n",
        "    \"\"\"\n",
        "    # 1. Get the model's generated text and extract the Prolog snippet\n",
        "    responses = [comp[0][\"content\"] for comp in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "\n",
        "    # 2. Retrieve reference numeric results (passed from dataset)\n",
        "    correct_values = numerical_result\n",
        "\n",
        "    # 3. Debug print for the first sample only\n",
        "    if len(responses) > 0:\n",
        "        question = prompts[0][-1][\"content\"] if (prompts and prompts[0]) else \"N/A\"\n",
        "        print(\n",
        "            \"-\" * 20,\n",
        "            f\"Question:\\n{question}\",\n",
        "            f\"\\nReference Prolog answer:\\n{answer[0]}\",\n",
        "            f\"\\nReference Numerical Result:\\n{correct_values[0]}\",\n",
        "            f\"\\nModel Response:\\n{responses[0]}\",\n",
        "            f\"\\nExtracted Code:\\n{extracted_responses[0]}\"\n",
        "        )\n",
        "\n",
        "    # 4. Execute the model's Prolog code with SWI-Prolog\n",
        "    model_values = []\n",
        "    for code in extracted_responses:\n",
        "        if code:\n",
        "            mv = execute_prolog_code(code)\n",
        "            if mv:\n",
        "                model_values.append(mv)\n",
        "            else:\n",
        "                model_values.append(None)\n",
        "                print(\"SWI-Prolog returned no output or an error.\")\n",
        "        else:\n",
        "            model_values.append(None)\n",
        "            print(\"No Prolog code extracted from the model.\")\n",
        "\n",
        "    # 5. Compare results and provide rewards\n",
        "    rewards = []\n",
        "    for mv, cv in zip(model_values, correct_values):\n",
        "        if mv is None or cv is None:\n",
        "            # Partial reward for at least attempting to generate some code\n",
        "            rewards.append(0.5)\n",
        "            print(\"Partial Reward: Model attempted code or code is None, no numeric match.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # If it's an unbound variable, e.g. \"_12345\", that's partial credit\n",
        "            if mv.startswith(\"_\"):\n",
        "                rewards.append(0.5)\n",
        "                print(f\"Unbound variable in Prolog output: {mv}\")\n",
        "                continue\n",
        "\n",
        "            mv_cleaned = mv.strip().split('\\n')[-1]\n",
        "            mv_float = float(mv_cleaned)\n",
        "            cv_float = float(cv)\n",
        "            print(f\"Model Value: {mv_float}, Correct Value: {cv_float}\")\n",
        "\n",
        "            if abs(mv_float - cv_float) < 1e-6:\n",
        "                # Full reward for correct numeric result\n",
        "                rewards.append(2.0)\n",
        "                print(\"Match: Model value matches correct value.\")\n",
        "            else:\n",
        "                # Partial reward for producing a numeric result, but not correct\n",
        "                rewards.append(1.0)\n",
        "                print(\"Partial Reward: Model generated a numeric result, but it's incorrect.\")\n",
        "        except Exception as e:\n",
        "            # Partial credit for at least generating code that runs\n",
        "            rewards.append(0.5)\n",
        "            print(f\"Error converting model output to float: {e}\\nModel: {mv}, Correct: {cv}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "def prolog_syntax_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Partial reward for including Prolog-specific patterns:\n",
        "      - ':-' (typical directives, e.g. :- use_module)\n",
        "      - 'solve('\n",
        "      - lines ending with '.'\n",
        "      - 'use_module(library(clpq))'\n",
        "    \"\"\"\n",
        "    pattern = r'(?::-|solve\\s*\\(|use_module|clpq|\\.\\s*$)'\n",
        "    rewards = []\n",
        "    for c in completions:\n",
        "        text = c[0][\"content\"]\n",
        "        hits = re.findall(pattern, text, re.MULTILINE)\n",
        "        # Simple approach: #hits * 0.2, capped at 1.0\n",
        "        score = min(len(hits) * 0.2, 1.0)\n",
        "        rewards.append(score)\n",
        "    return rewards\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a CoT-like XML format.\"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.search(pattern, r, flags=re.DOTALL) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text: str) -> float:\n",
        "    \"\"\"\n",
        "    A custom function that attempts to parse how well the output\n",
        "    adheres to your <reasoning>...</reasoning> <answer>...</answer> blocks.\n",
        "    \"\"\"\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1) * 0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRPOConfig and GRPOTrainer"
      ],
      "metadata": {
        "id": "uF4n7xGh5INA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CBhiRud03uD"
      },
      "outputs": [],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    seed=42,\n",
        "    use_vllm = True, # use vLLM for fast inference!\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size = 8,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 8, # Decrease if out of memory\n",
        "    max_prompt_length = 256,\n",
        "    max_completion_length = 768,\n",
        "    num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    #max_steps = 1000,\n",
        "    save_steps = 250,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"wandb\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqTcnqzSnWz_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        prolog_syntax_reward_func,\n",
        "        correctness_reward_func,\n",
        "    ],\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# Save the LoRA Adapter\n",
        "model.save_lora(\"grpo_saved_lora\")\n",
        "\n",
        "# Merge to 16bit - Replace with your HF Repo, Model Name, and HF Token\n",
        "if True: model.save_pretrained_merged(\"qwen2.5-3b-grpo-1.75k-gsm8k-sp-struct-rwd1\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if True: model.push_to_hub_merged(\"niklasm222/qwen2.5-3b-grpo-1.75k-gsm8k-sp-struct-rwd1\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
      ]
    }
  ]
}
